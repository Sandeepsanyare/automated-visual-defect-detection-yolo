@article{Badmos2020,
  author  = {Badmos, T. and others},
  journal = {Procedia Manufacturing},
  title   = {Comparison of supervised and unsupervised learning for industrial defect detection},
  year    = {2020},
  volume  = {51},
  doi     = {10.63313/jcsft.9001}
}

@inproceedings{bergmann2019mvtec,
  author    = {Bergmann, Paul and others},
  booktitle = {CVPR},
  doi       = {10.1109/cvpr.2019.00982},
  title     = {MVTec AD — A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection},
  year      = {2019}
}

@article{bochkovskiy2020yolov4,
  author  = {Bochkovskiy, Alexey and others},
  journal = {arXiv preprint arXiv:2004.10934},
  title   = {Yolov4: Optimal speed and accuracy of object detection},
  year    = {2020}
}

@article{ciampoli2019defect,
  author  = {Ciampoli, A and others},
  journal = {Procedia Structural Integrity},
  title   = {Defect detection and characterization in rolled products},
  volume  = {24},
  year    = {2019}
}

@article{czimmermann2020visual,
  author  = {Czimmermann, Tam{\'a}s and others},
  journal = {IEEE Access},
  title   = {Visual quality inspection and fine-grained defect detection: A survey},
  volume  = {8},
  year    = {2020}
}

@inproceedings{dhariwal2021diffusion,
  author    = {Dhariwal, Prafulla and Nichol, Alexander},
  booktitle = {NeurIPS},
  title     = {Diffusion models beat gans on image synthesis},
  year      = {2021}
}

@article{dosovitskiy2020image,
  author  = {Dosovitskiy, Alexey and others},
  journal = {ICLR},
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  year    = {2020}
}

@article{elfwing2018sigmoid,
  author  = {Elfwing, Stefan and others},
  doi     = {10.1016/j.neunet.2017.12.012},
  journal = {Neural Networks},
  title   = {Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  volume  = {107},
  year    = {2018}
}

@article{Feng2021,
  author        = {Feng, Chengjian and Zhong, Yujie and Gao, Yu and Scott, Matthew R. and Huang, Weilin},
  title         = {TOOD: Task-aligned One-stage Object Detection},
  year          = {2021},
  month         = aug,
  abstract      = {One-stage object detection is commonly implemented by optimizing two sub-tasks: object classification and localization, using heads with two parallel branches, which might lead to a certain level of spatial misalignment in predictions between the two tasks. In this work, we propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the two tasks in a learning-based manner. First, we design a novel Task-aligned Head (T-Head) which offers a better balance between learning task-interactive and task-specific features, as well as a greater flexibility to learn the alignment via a task-aligned predictor. Second, we propose Task Alignment Learning (TAL) to explicitly pull closer (or even unify) the optimal anchors for the two tasks during training via a designed sample assignment scheme and a task-aligned loss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a 51.1 AP at single-model single-scale testing. This surpasses the recent one-stage detectors by a large margin, such as ATSS (47.7 AP), GFL (48.2 AP), and PAA (49.0 AP), with fewer parameters and FLOPs. Qualitative results also demonstrate the effectiveness of TOOD for better aligning the tasks of object classification and localization. Code is available at https://github.com/fcjian/TOOD.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2108.07755},
  eprint        = {2108.07755},
  file          = {:Feng2021 - TOOD_ Task Aligned One Stage Object Detection.pdf:PDF:https\://arxiv.org/pdf/2108.07755v3},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv}
}

@article{ge2021yolox,
  author = {Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},
  title  = {Yolox: exceeding yolo series in 2021},
  year   = {2021}
}

@article{Gevorgyan2022,
  author        = {Gevorgyan, Zhora},
  title         = {SIoU Loss: More Powerful Learning for Bounding Box Regression},
  year          = {2022},
  month         = may,
  abstract      = {The effectiveness of Object Detection, one of the central problems in computer vision tasks, highly depends on the definition of the loss function - a measure of how accurately your ML model can predict the expected outcome. Conventional object detection loss functions depend on aggregation of metrics of bounding box regression such as the distance, overlap area and aspect ratio of the predicted and ground truth boxes (i.e. GIoU, CIoU, ICIoU etc). However, none of the methods proposed and used to date considers the direction of the mismatch between the desired ground box and the predicted, "experimental" box. This shortage results in slower and less effective convergence as the predicted box can "wander around" during the training process and eventually end up producing a worse model. In this paper a new loss function SIoU was suggested, where penalty metrics were redefined considering the angle of the vector between the desired regression. Applied to conventional Neural Networks and datasets it is shown that SIoU improves both the speed of training and the accuracy of the inference. The effectiveness of the proposed loss function was revealed in a number of simulations and tests.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2205.12740},
  eprint        = {2205.12740},
  file          = {:Gevorgyan2022 - SIoU Loss_ More Powerful Learning for Bounding Box Regression.pdf:PDF:https\://arxiv.org/pdf/2205.12740v1},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, I.2; I.4},
  primaryclass  = {cs.CV},
  publisher     = {arXiv}
}

@article{ghorai2013automatic,
  author  = {Ghorai, S. and others},
  doi     = {10.1109/tim.2012.2218677},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  title   = {Automatic Defect Detection on Hot-Rolled Flat Steel Products},
  volume  = {62},
  year    = {2013}
}

@inproceedings{goodfellow2014generative,
  author    = {Goodfellow, Ian and others},
  booktitle = {NeurIPS},
  doi       = {10.1007/978-3-658-40442-0_9},
  title     = {Generative Adversarial Nets},
  year      = {2023}
}

@book{goodfellow2016deep,
  author    = {Goodfellow, Ian and others},
  publisher = {MIT press},
  title     = {Deep Learning},
  year      = {2016}
}

@article{Han2019,
  author        = {Han, Kai and Wang, Yunhe and Tian, Qi and Guo, Jianyuan and Xu, Chunjing and Xu, Chang},
  title         = {GhostNet: More Features from Cheap Operations},
  year          = {2019},
  month         = nov,
  abstract      = {Deploying convolutional neural networks (CNNs) on embedded devices is difficult due to the limited memory and computation resources. The redundancy in feature maps is an important characteristic of those successful CNNs, but has rarely been investigated in neural architecture design. This paper proposes a novel Ghost module to generate more feature maps from cheap operations. Based on a set of intrinsic feature maps, we apply a series of linear transformations with cheap cost to generate many ghost feature maps that could fully reveal information underlying intrinsic features. The proposed Ghost module can be taken as a plug-and-play component to upgrade existing convolutional neural networks. Ghost bottlenecks are designed to stack Ghost modules, and then the lightweight GhostNet can be easily established. Experiments conducted on benchmarks demonstrate that the proposed Ghost module is an impressive alternative of convolution layers in baseline models, and our GhostNet can achieve higher recognition performance (e.g. $75.7\%$ top-1 accuracy) than MobileNetV3 with similar computational cost on the ImageNet ILSVRC-2012 classification dataset. Code is available at https://github.com/huawei-noah/ghostnet},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1911.11907},
  eprint        = {1911.11907},
  file          = {:Han2019 - GhostNet_ More Features from Cheap Operations.pdf:PDF:https\://arxiv.org/pdf/1911.11907v2},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv}
}

@article{Haralick1973,
  author    = {Haralick, Robert M. and Shanmugam, K. and Dinstein, Its’Hak},
  journal   = {IEEE Transactions on Systems, Man, and Cybernetics},
  title     = {Textural Features for Image Classification},
  year      = {1973},
  issn      = {2168-2909},
  month     = {November},
  number    = {6},
  pages     = {610--621},
  volume    = {SMC-3},
  doi       = {10.1109/tsmc.1973.4309314},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)}
}

@article{haralick1979statistical,
  author  = {Haralick, Robert M},
  doi     = {10.1109/proc.1979.11328},
  journal = {Proceedings of the IEEE},
  number  = {5},
  title   = {Statistical and structural approaches to texture},
  volume  = {67},
  year    = {1979}
}

@article{He2015,
  author   = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
  year     = {2015},
  number   = {9},
  pages    = {1904-1916},
  volume   = {37},
  doi      = {10.1109/TPAMI.2015.2389824},
  keywords = {Training;Feature extraction;Accuracy;Convolutional codes;Agriculture;Testing;Vectors;Convolutional Neural Networks;Spatial Pyramid Pooling;Image Classification;Object Detection;Convolutional neural networks;spatial pyramid pooling;image classification;object detection}
}

@inproceedings{he2016deep,
  author    = {He, Kaiming and others},
  booktitle = {CVPR},
  doi       = {10.1109/cvpr.2016.90},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016}
}

@inproceedings{ho2020denoising,
  author    = {Ho, Jonathan and others},
  booktitle = {NeurIPS},
  doi       = {10.2139/ssrn.5358689},
  title     = {Riemannian Denoising Diffusion Probabilistic Models},
  year      = {2025}
}

@inproceedings{hou2021coordinate,
  author    = {Hou, Qibin and others},
  booktitle = {CVPR},
  doi       = {10.1109/cvpr46437.2021.01350},
  title     = {Coordinate Attention for Efficient Mobile Network Design},
  year      = {2021}
}

@misc{https://doi.org/10.48550/arxiv.2402.13616,
  author    = {Wang, Chien-Yao and Yeh, I-Hau and Liao, Hong-Yuan Mark},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2402.13616},
  journal   = {arXiv preprint arXiv:2402.13616},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  title     = {YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information},
  year      = {2024}
}

@misc{https://doi.org/10.48550/arxiv.2410.17725,
  author    = {Khanam, Rahima and Hussain, Muhammad},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.48550/ARXIV.2410.17725},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
  title     = {YOLOv11: An Overview of the Key Architectural Enhancements},
  year      = {2024}
}

@article{industry4,
  author    = {Lasi, Heiner and Fettke, Peter and Kemper, Hans-Georg and Feld, Thomas and Hoffmann, Michael},
  title     = {Industry 4.0},
  journal   = {Business \& Information Systems Engineering},
  volume    = {6},
  number    = {4},
  pages     = {239--242},
  year      = {2014},
  publisher = {Springer}
}

@article{jocher2020yolov5,
  author  = {Jocher, Glenn},
  journal = {GitHub repository},
  title   = {Ultralytics YOLOv5},
  year    = {2020}
}

@misc{jocher2023yolo,
  title        = {{YOLO} by {Ultralytics}},
  author       = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
  year         = {2023},
  howpublished = {\url{https://github.com/ultralytics/ultralytics}},
  note         = {Accessed: 2023-01-19}
}

@article{jocher2024yolo11,
  author  = {Jocher, Glenn and others},
  journal = {GitHub repository},
  note    = {https://github.com/ultralytics/ultralytics},
  title   = {Ultralytics YOLO11},
  year    = {2024}
}

@article{johnson2019survey,
  author  = {Johnson, Justin M and Khoshgoftaar, Taghi M},
  doi     = {10.1186/s40537-019-0192-5},
  journal = {Journal of Big Data},
  title   = {Survey on deep learning with class imbalance},
  volume  = {6},
  year    = {2019}
}

@article{kagermann2013recommendations,
  author  = {Kagermann, Henning and Wahlster, Wolfgang and Helbig, Johannes},
  journal = {Final report of the Industrie 4.0 Working Group},
  title   = {Recommendations for implementing the strategic initiative INDUSTRIE 4.0},
  year    = {2013}
}

@article{kumar2002defect,
  author  = {Kumar, A. and Pang, G.K.H.},
  doi     = {10.1109/28.993164},
  journal = {IEEE Transactions on Industry Applications},
  number  = {2},
  title   = {Defect detection in textured materials using Gabor filters},
  volume  = {38},
  year    = {2002}
}

@article{lecun2015deep,
  author  = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  doi     = {10.1038/nature14539},
  journal = {Nature},
  title   = {Deep learning},
  volume  = {521},
  year    = {2015}
}

@article{lee2024hybriddc,
  author  = {Lee, J. and others},
  doi     = {10.3390/electronics13224467},
  journal = {Electronics},
  title   = {Hybrid-DC: A Hybrid Framework Using ResNet-50 and Vision Transformer for Steel Surface Defect Classification in the Rolling Process},
  volume  = {13},
  year    = {2024}
}

@misc{Li2020,
  author    = {Li, Xiang and Wang, Wenhai and Wu, Lijun and Chen, Shuo and Hu, Xiaolin and Li, Jun and Tang, Jinhui and Yang, Jian},
  title     = {Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2006.04388},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv}
}

@inproceedings{li2022uniformer,
  author    = {Li, Kunchang and others},
  booktitle = {ICLR},
  doi       = {10.1109/tpami.2023.3282631},
  title     = {UniFormer: Unifying Convolution and Self-Attention for Visual Recognition},
  year      = {2023}
}

@misc{li2022yolov6,
  author    = {Li, Chuyi and Li, Lulu and Jiang, Hongliang and Weng, Kaiheng and Geng, Yifei and Li, Liang and Ke, Zaidan and Li, Qingyuan and Cheng, Meng and Nie, Weiqiang and Li, Yiduo and Zhang, Bo and Liang, Yufei and Zhou, Linyuan and Xu, Xiaoming and Chu, Xiangxiang and Wei, Xiaoming and Wei, Xiaolin},
  title     = {YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2209.02976},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv}
}

@article{Lin2014,
  author        = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
  title         = {Microsoft COCO: Common Objects in Context},
  year          = {2014},
  month         = may,
  abstract      = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1405.0312},
  eprint        = {1405.0312},
  file          = {:Lin2014 - Microsoft COCO_ Common Objects in Context.pdf:PDF:https\://arxiv.org/pdf/1405.0312v3},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv}
}

@inproceedings{lin2017feature,
  author    = {Lin, Tsung-Yi and others},
  booktitle = {CVPR},
  doi       = {10.1109/cvpr.2017.106},
  title     = {Feature Pyramid Networks for Object Detection},
  year      = {2017}
}

@inproceedings{liu2018path,
  author    = {Liu, Shu and others},
  booktitle = {CVPR},
  doi       = {10.1109/cvpr.2018.00913},
  title     = {Path Aggregation Network for Instance Segmentation},
  year      = {2018}
}

@inproceedings{liu2021swin,
  author    = {Liu, Ze and others},
  booktitle = {ICCV},
  doi       = {10.1109/iccv48922.2021.00986},
  title     = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  year      = {2021}
}

@inproceedings{liu2023dream,
  title     = {{DREAM}: Efficient Dataset Distillation by Representative Matching},
  author    = {Liu, Yanqing and Gu, Jianyang and Wang, Kai and Zhu, Zheng and Jiang, Wei and You, Yang},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages     = {17314--17324},
  year      = {2023}
}

@article{liu2023survey,
  author  = {Liu, Z. and others},
  journal = {IEEE Transactions on Industrial Informatics},
  title   = {A survey of vision transformers in industrial inspection},
  year    = {2023}
}

@article{luo2020automated,
  author  = {Luo, Quande and others},
  journal = {Journal of Intelligent Manufacturing},
  title   = {Automated vision-based surface defect detection: A review},
  volume  = {31},
  year    = {2020}
}

@article{lv2020deep,
  author  = {Lv, X. and others},
  doi     = {10.1109/imcec46724.2019.8984019},
  journal = {IEEE Transactions on Industrial Informatics},
  title   = {Deep learning for surface defect detection on industrial products},
  year    = {2020}
}

@article{malamas2003survey,
  author  = {Malamas, E.N. and others},
  doi     = {10.1016/s0262-8856(02)00152-x},
  journal = {Image and Vision Computing},
  number  = {2},
  title   = {A survey on industrial vision systems, applications and tools},
  volume  = {21},
  year    = {2003}
}

@article{neu-det,
  author    = {Song, Kechen and Yan, Yunhui},
  title     = {A noise robust method based on completed local binary patterns for hot-rolled steel strip surface defects},
  journal   = {Applied Surface Science},
  volume    = {285},
  pages     = {858--864},
  year      = {2013},
  publisher = {Elsevier}
}

@inproceedings{optuna,
  author    = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  title     = {Optuna: A Next-generation Hyperparameter Optimization Framework},
  booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year      = {2019},
  pages     = {2623--2631},
  publisher = {ACM}
}

@article{powell2022zero,
  author  = {Powell, Daryl and others},
  journal = {Computers in Industry},
  title   = {Zero defect manufacturing: A self-optimising perspective},
  volume  = {136},
  year    = {2022}
}

@article{psarommatis2020zero,
  author  = {Psarommatis, Foivos and others},
  doi     = {10.1080/00207543.2019.1605228},
  journal = {International Journal of Production Research},
  number  = {1},
  title   = {Zero defect manufacturing: state-of-the-art review, shortcomings and future directions in research},
  volume  = {58},
  year    = {2019}
}

@article{psarommatis2022zero,
  author  = {Psarommatis, F. and others},
  journal = {International Journal of Production Research},
  title   = {Zero-defect manufacturing: Current state and future trends},
  year    = {2022}
}

@misc{redmon2016you,
  author    = {Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
  title     = {You only look once: unified, real-time object detection},
  year      = {2016},
  doi       = {10.1109/cvpr.2016.91},
  journal   = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {779-788},
  publisher = {IEEE}
}

@misc{redmon2017yolo9000,
  author    = {Joseph Redmon and Ali Farhadi},
  title     = {YOLO9000: Better, Faster, Stronger},
  year      = {2017},
  doi       = {10.1109/cvpr.2017.690},
  journal   = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages     = {6517-6525},
  publisher = {IEEE}
}

@article{redmon2018yolov3,
  author  = {Redmon, Joseph and Farhadi, Ali},
  journal = {arXiv preprint arXiv:1804.02767},
  title   = {Yolov3: An incremental improvement},
  year    = {2018}
}


@article{Ren2017,
  author    = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {Faster r-cnn: towards real-time object detection with region proposal networks},
  year      = {2017},
  issn      = {0162-8828},
  number    = {6},
  volume    = {39},
  doi       = {10.1109/tpami.2016.2577031},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)}
}

@article{rezende2015variational,
  author  = {Rezende, Danilo and Mohamed, Shakir},
  journal = {ICML},
  title   = {Variational inference with normalizing flows},
  year    = {2015}
}

@inproceedings{roth2022towards,
  author    = {Roth, Karsten and others},
  booktitle = {CVPR},
  doi       = {10.1109/cvpr52688.2022.01392},
  title     = {Towards Total Recall in Industrial Anomaly Detection},
  year      = {2022}
}

@article{see2012visual,
  author  = {See, J.E.},
  doi     = {10.1177/0018720815602389},
  journal = {Human Factors},
  number  = {6},
  title   = {Visual Inspection Reliability for Precision Manufactured Parts},
  volume  = {54},
  year    = {2015}
}

@article{Simonyan2014,
  author        = {Simonyan, Karen and Zisserman, Andrew},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  year          = {2014},
  month         = sep,
  abstract      = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1409.1556},
  eprint        = {1409.1556},
  file          = {:Simonyan2014 - Very Deep Convolutional Networks for Large Scale Image Recognition.pdf:PDF:https\://arxiv.org/pdf/1409.1556v6},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv}
}

@article{Song2013,
  author    = {Kechen Song and Yunhui Yan},
  doi       = {10.1016/j.apsusc.2013.09.002},
  issn      = {0169-4332},
  journal   = {Applied Surface Science},
  pages     = {858-864},
  publisher = {Elsevier BV},
  title     = {A noise robust method based on completed local binary patterns for hot-rolled steel strip surface defects},
  volume    = {285},
  year      = {2013}
}

@article{soviany2018curriculum,
  author  = {Soviany, Petru and Ionescu, Radu Tudor},
  journal = {arXiv preprint},
  title   = {Curriculum learning for two-stage object detection},
  year    = {2018}
}

@article{tao2018automatic,
  author  = {Tao, Xian and others},
  doi     = {10.3390/app8091575},
  journal = {Applied Sciences},
  title   = {Automatic Metallic Surface Defect Detection and Recognition with Convolutional Neural Networks},
  volume  = {8},
  year    = {2018}
}

@misc{Tong2023,
  author    = {Tong, Zanjia and Chen, Yuhang and Xu, Zewei and Yu, Rong},
  title     = {Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism},
  year      = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2301.10051},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv}
}

@article{tsai2010automated,
  author  = {Tsai, D.M. and Hsieh, C.Y.},
  doi     = {10.1016/s0262-8856(03)00007-6},
  journal = {Image and Vision Computing},
  title   = {Automated surface inspection for statistical textures},
  volume  = {18},
  year    = {2003}
}

@article{tuceryan1993texture,
  author  = {Tuceryan, Mihran and Jain, Anil K},
  doi     = {10.1142/9789814343138_0010},
  journal = {Handbook of pattern recognition and computer vision},
  title   = {Texture analysis},
  year    = {1993}
}

@inproceedings{Varghese_2024,
  author    = {Varghese, Rejin and M., Sambath},
  booktitle = {2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS)},
  doi       = {10.1109/adics58448.2024.10533619},
  journal   = {https://github.com/ultralytics/ultralytics},
  month     = {April},
  pages     = {1--6},
  publisher = {IEEE},
  title     = {YOLOv8: A Novel Object Detection Algorithm with Enhanced Performance and Robustness},
  year      = {2024}
}

@inproceedings{vaswani2017attention,
  author    = {Vaswani, Ashish and others},
  booktitle = {NeurIPS},
  doi       = {10.65215/nxvz2v36},
  title     = {Attention Is All You Need},
  year      = {2025}
}

@article{Wang2019,
  author        = {Wang, Chien-Yao and Liao, Hong-Yuan Mark and Yeh, I-Hau and Wu, Yueh-Hua and Chen, Ping-Yang and Hsieh, Jun-Wei},
  title         = {CSPNet: A New Backbone that can Enhance Learning Capability of CNN},
  year          = {2019},
  month         = nov,
  abstract      = {Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet. Source code is at https://github.com/WongKinYiu/CrossStagePartialNetworks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1911.11929},
  eprint        = {1911.11929},
  file          = {:Wang2019 - CSPNet_ a New Backbone That Can Enhance Learning Capability of CNN.pdf:PDF:https\://arxiv.org/pdf/1911.11929v1},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv}
}

@article{Wang2019a,
  author        = {Wang, Jiaqi and Chen, Kai and Xu, Rui and Liu, Ziwei and Loy, Chen Change and Lin, Dahua},
  title         = {CARAFE: Content-Aware ReAssembly of FEatures},
  year          = {2019},
  month         = may,
  abstract      = {Feature upsampling is a key operation in a number of modern convolutional network architectures, e.g. feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE has several appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit sub-pixel neighborhood, CARAFE can aggregate contextual information within a large receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly. (3) Lightweight and fast to compute. CARAFE introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation and inpainting. CARAFE shows consistent and substantial gains across all the tasks (1.2%, 1.3%, 1.8%, 1.1db respectively) with negligible computational overhead. It has great potential to serve as a strong building block for future research. It has great potential to serve as a strong building block for future research. Code and models are available at https://github.com/open-mmlab/mmdetection.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1905.02188},
  eprint        = {1905.02188},
  file          = {:Wang2019a - CARAFE_ Content Aware ReAssembly of FEatures.pdf:PDF:https\://arxiv.org/pdf/1905.02188v3},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv}
}

@article{wang2023yolov7,
  author        = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  title         = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  year          = {2022},
  month         = jul,
  abstract      = {YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed and 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/WongKinYiu/yolov7.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2207.02696},
  eprint        = {2207.02696},
  file          = {:Wang2022 - YOLOv7_ Trainable Bag of Freebies Sets New State of the Art for Real Time Object Detectors.pdf:PDF:https\://arxiv.org/pdf/2207.02696v1},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv}
}

@misc{Wang2024,
  author    = {Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang},
  title     = {YOLOv10: Real-Time End-to-End Object Detection},
  year      = {2024},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {10.48550/ARXIV.2405.14458},
  journal   = {arXiv preprint arXiv:2405.14458},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv}
}

@article{Wang2025,
  author        = {Wang, Fang and Li, Huitao and Chao, Wenhan and Zhuo, Zheng and Ji, Yiran and Peng, Chang and Sun, Yupeng},
  title         = {E-convnext: a lightweight and efficient convnext variant with cross-stage partial connections},
  year          = {2025},
  month         = aug,
  abstract      = {Many high-performance networks were not designed with lightweight application scenarios in mind from the outset, which has greatly restricted their scope of application. This paper takes ConvNeXt as the research object and significantly reduces the parameter scale and network complexity of ConvNeXt by integrating the Cross Stage Partial Connections mechanism and a series of optimized designs. The new network is named E-ConvNeXt, which can maintain high accuracy performance under different complexity configurations. The three core innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network (CSPNet) with ConvNeXt and adjusting the network structure, which reduces the model's network complexity by up to 80%; (2) Optimizing the Stem and Block structures to enhance the model's feature expression capability and operational efficiency; (3) Replacing Layer Scale with channel attention. Experimental validation on ImageNet classification demonstrates E-ConvNeXt's superior accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at 0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer learning tests on object detection tasks further confirm its generalization capability.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2508.20955},
  eprint        = {2508.20955},
  file          = {:Wang2025 - E ConvNeXt_ a Lightweight and Efficient ConvNeXt Variant with Cross Stage Partial Connections.pdf:PDF:https\://arxiv.org/pdf/2508.20955v1},
  keywords      = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  primaryclass  = {cs.CV},
  publisher     = {arXiv}
}

@misc{Woo2018,
  author    = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  title     = {CBAM: Convolutional Block Attention Module},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1807.06521},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv}
}

@inproceedings{wu2021cvt,
  author    = {Wu, Haiping and others},
  booktitle = {ICCV},
  doi       = {10.1109/iccv48922.2021.00009},
  title     = {CvT: Introducing Convolutions to Vision Transformers},
  year      = {2021}
}

@article{xie2008review,
  author  = {Xie, X.},
  journal = {World Congress on Intelligent Control and Automation},
  title   = {A review of classification algorithms for surface defect detection},
  year    = {2008}
}

@article{xie2020review,
  author  = {Xie, L. and others},
  doi     = {10.5565/rev/elcvia.268},
  journal = {Electronic Imaging},
  title   = {A review of recent advances in surface defect detection using deep learning techniques},
  year    = {2020}
}

@inproceedings{yang2021simam,
  author    = {Yang, Lingxiao and others},
  booktitle = {ICML},
  doi       = {10.36227/techrxiv.22662601.v1},
  title     = {Simam: A simple, parameter-free attention module for convolutional neural networks},
  year      = {2021}
}

@software{yolov8,
  author  = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
  title   = {Ultralytics YOLOv8},
  year    = {2023},
  url     = {https://github.com/ultralytics/ultralytics},
  version = {8.0.0}
}

@article{yu2021fastflow,
  author  = {Yu, Jiawei and others},
  doi     = {10.1109/wacv51458.2022.00188},
  journal = {arXiv preprint arXiv:2111.07677},
  title   = {Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows},
  year    = {2021}
}

@article{zeiler2014visualizing,
  author  = {Zeiler, Matthew D and Fergus, Rob},
  doi     = {10.1007/978-3-319-10590-1_53},
  journal = {ECCV},
  title   = {Visualizing and Understanding Convolutional Networks},
  year    = {2014}
}

@misc{Zhang2017,
  author    = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
  title     = {mixup: Beyond Empirical Risk Minimization},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.1710.09412},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv}
}

@article{zhang2023generative,
  author  = {Zhang, J. and others},
  journal = {arXiv preprint},
  title   = {Generative AI for industrial anomaly detection},
  year    = {2023}
}

@article{zhao2024fewshot,
  author  = {Zhao, Y. and others},
  journal = {Pattern Recognition},
  title   = {Few-shot defect detection: A survey},
  year    = {2024}
}

@article{Zheng2020,
  author    = {Zheng, Zhaohui and Wang, Ping and Liu, Wei and Li, Jinze and Ye, Rongguang and Ren, Dongwei},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title     = {Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression},
  year      = {2020},
  issn      = {2159-5399},
  month     = apr,
  number    = {07},
  pages     = {12993--13000},
  volume    = {34},
  doi       = {10.1609/aaai.v34i07.6999},
  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)}
}

@Article{Badmos2019,
  author    = {Badmos, Olatomiwa and Kopp, Andreas and Bernthaler, Timo and Schneider, Gerhard},
  journal   = {Journal of Intelligent Manufacturing},
  title     = {Image-based defect detection in lithium-ion battery electrode using convolutional neural networks},
  year      = {2019},
  issn      = {1572-8145},
  month     = aug,
  number    = {4},
  pages     = {885--897},
  volume    = {31},
  doi       = {10.1007/s10845-019-01484-x},
  publisher = {Springer Science and Business Media LLC},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectoryLatex-shivomg-cachy:/home/shivomg/DefectDetectionYOLO/thesis/sections;}
